5 p-values: ['0.0', '0.0', '0.0', '0.0', '0.0']
50 p-values: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']
Accuracy for 1k: 0.2898
Accuracy for full dataset: 0.3947
Chosen feature intersection: [29 35 83 96]
Top-5 at higher: [29 35 83 86 96]

(a) (Note, index starts at 0) The names for those features are liwc_AllPunc, liwc_OtherP, liwc_motion, liwc_relativ. Punctation and relative clauses could express the speaker's feeling.and emotion, so they are kind of important'
(b) More data generally helps the model to better separate the classes, thus p-values generally lower.
(c) The names for top 5 features chosen for 32K training set is liwc_AllPunc, liwc_OtherP, liwc_motion, liwc_netspeak, liwc_relativ. Those features kind of related to the emotion of speak, which helps to separate the classes.